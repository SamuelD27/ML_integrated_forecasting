â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         B200 TRAINING - QUICK START
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HARDWARE: 1x NVIDIA B200 (192GB VRAM)
DATASET: 500 S&P 500 tickers Ã— 15 years = ~7.5M rows
MODEL: 100M parameters (1024H Ã— 12L Ã— 16A Transformer)
TIME: 8-10 hours total
COST: ~$800-1000 ($100-120/hr Ã— 10 hours)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            ONE-LINE DEPLOYMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SSH into RunPod B200 instance and run:

cd /workspace && \
git clone https://github.com/SamuelD27/ML_integrated_forecasting.git code && \
cd code/runpod_setup && \
bash deploy_b200.sh

That's it! The script will:
  1. Setup environment (~5 min)
  2. Fetch 500 tickers Ã— 15 years (~30-45 min)
  3. Train 100M param model (~8-10 hours)
  4. Save results

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           MANUAL STEP-BY-STEP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

If you prefer manual control:

# 1. Clone repository
cd /workspace
git clone https://github.com/SamuelD27/ML_integrated_forecasting.git code
cd code

# 2. Install dependencies
pip install torch pytorch-lightning pandas numpy pyarrow yfinance \
            lightgbm scikit-learn tensorboard tqdm pyyaml

# 3. Fetch data (~30-45 min)
cd runpod_setup
python fetch_sp500_data.py

# 4. Train model (~8-10 hours)
python train_b200.py

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              MONITORING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

In another terminal:

# GPU utilization (should be >90%)
watch -n 1 nvidia-smi

# Training log
tail -f /workspace/output/training.log

# TensorBoard (if port forwarded)
tensorboard --logdir /workspace/output/logs --port 6006

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          CONFIGURATION DETAILS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model Architecture:
  Input dim: 9 features (OHLCV + technical indicators)
  Hidden size: 1024 (4x larger than baseline)
  Layers: 12 (2x deeper than baseline)
  Attention heads: 16 (2x more than baseline)
  Feed-forward: 4096 (2x wider than baseline)
  Total params: ~100M (5x larger than baseline)
  Dropout: 0.2

Training Configuration:
  Batch size: 16,384 (MASSIVE - only possible on B200)
  Gradient accumulation: 1 (no need with large batch)
  Max epochs: 150
  Learning rate: 0.0005 (lower for large batch)
  Warmup epochs: 5
  Patience: 25 (early stopping)
  Precision: BF16-mixed (optimal for B200)
  Compile: True (torch.compile for 30% speedup)

Data Configuration:
  Train split: 80%
  Val split: 10%
  Test split: 10%
  Num workers: 32 (B200 can handle many)
  Sequence length: 120 days
  Forecast horizon: 20 days

Checkpointing (MINIMAL):
  Save top K: 1 (only best model)
  Save last: False
  Every N epochs: 10
  Monitor: val_loss

Dataset Details:
  Tickers: 500 (complete S&P 500)
  Timeframe: 15 years
  Interval: Daily
  Features: 18 technical indicators
  Total rows: ~7.5M
  File size: ~300MB (parquet)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                             EXPECTED RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Training:
  Samples: ~6.3M training samples
  Steps per epoch: ~385 (6.3M / 16,384)
  Time per epoch: ~3-4 minutes
  Total time: ~8-10 hours (150 epochs with early stopping)

Performance:
  Val loss: <0.0008 (better than baseline)
  Test accuracy: 58-62% directional (excellent for finance)
  Model size: ~400MB (fp32)
  Inference: <1ms per prediction

Outputs:
  Best checkpoint: /workspace/output/checkpoints/best-*.ckpt
  TensorBoard logs: /workspace/output/logs/
  Training log: /workspace/output/training.log
  Results JSON: /workspace/output/training_results.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           DOWNLOAD RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

From your local machine:

# Download everything
scp -r runpod@<ip>:/workspace/output ./b200_results

# Download just the checkpoint
scp runpod@<ip>:/workspace/output/checkpoints/best-*.ckpt ./best_model.ckpt

# Download just the logs
scp -r runpod@<ip>:/workspace/output/logs ./tensorboard_logs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           COST BREAKDOWN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

B200 hourly rate: $100-120/hr (varies by provider)

Phase breakdown:
  Setup: 5 min = $8-10
  Data fetch: 45 min = $75-90
  Training: 9 hrs = $900-1080
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: ~$980-1180

Cost optimization tips:
  âœ“ Use spot instances (50-70% cheaper, may be interrupted)
  âœ“ Stop instance IMMEDIATELY after training
  âœ“ Don't leave idle
  âœ“ Download results and terminate

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            TROUBLESHOOTING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Out of Memory:
  â†’ Reduce batch_size from 16384 to 8192 in train_b200.py
  â†’ Or enable gradient accumulation: accumulate_grad_batches=2

Slow training:
  â†’ Verify BF16 precision is enabled (not FP32)
  â†’ Check GPU util: nvidia-smi (should be >90%)
  â†’ Ensure torch.compile is enabled

Data fetch fails:
  â†’ Some tickers may fail (normal)
  â†’ Training proceeds with successfully fetched data
  â†’ Check: grep "Failed" /workspace/data/training/metadata.json

Training crashes:
  â†’ Check CUDA memory: nvidia-smi
  â†’ View error log: tail -100 /workspace/output/training.log
  â†’ Verify Python 3.11+ (not 3.12+)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         WHY B200 IS OPTIMAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Advantages:
  âœ“ 192GB VRAM â†’ Can train 100M+ param models
  âœ“ BF16 support â†’ Better numeric stability than FP16
  âœ“ 5,000 TFLOPS â†’ 2-3x faster than H100
  âœ“ 8 TB/s bandwidth â†’ No memory bottleneck
  âœ“ Batch 16,384 â†’ Better convergence, fewer steps

Comparison:
  1x B200 (16K batch) â‰ˆ 4x H100 (4K batch each)
  1x B200 cost: $1000
  4x H100 cost: $1600
  â†’ B200 is 37% cheaper for same performance!

Perfect for:
  âœ“ Large models (50M-500M params)
  âœ“ Massive datasets (5M+ rows)
  âœ“ Single long training session
  âœ“ Research & experimentation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              NEXT STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

After training completes:

1. Download checkpoint
   scp runpod@<ip>:/workspace/output/checkpoints/best-*.ckpt ./

2. Load in PyTorch
   from train_b200 import TransformerForecastModel
   model = TransformerForecastModel.load_from_checkpoint('best-*.ckpt')

3. Run backtests on held-out data

4. Integrate into dashboard

5. Monitor prediction accuracy in production

6. Retrain every 3-6 months with fresh data

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Questions? Check the full README: runpod_setup/README.md

Happy training! ğŸš€
