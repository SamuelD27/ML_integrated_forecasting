# Hybrid Deep Learning Trading Model - Training Configuration
# ============================================================

# Data Configuration
data:
  # Data paths
  base_name: "last_fetch"
  features_dir: "data/features"

  # Time series parameters
  sequence_length: 60  # Number of time steps to look back
  forecast_horizon: 1  # Days ahead to predict

  # Train/Validation/Test splits
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

  # Walk-forward validation
  walk_forward:
    enabled: true
    train_window: 252  # 1 year of trading days
    val_window: 63     # 3 months
    step_size: 21      # 1 month

  # Feature selection
  feature_groups:
    price_features: true
    technical_indicators: true
    frequency_domain: true
    microstructure: true
    volatility: true
    sentiment: true
    cross_sectional: false  # Set to true if market data available

# Model Architecture
model:
  # CNN configuration
  cnn:
    filters: [64, 128, 256]
    kernel_sizes: [3, 5, 7]
    dropout: 0.3

  # LSTM configuration
  lstm:
    hidden_units: 256
    num_layers: 2
    bidirectional: true
    dropout: 0.2
    use_attention: true

  # Transformer configuration
  transformer:
    d_model: 512
    n_heads: 8
    num_layers: 4
    dim_feedforward: 2048
    dropout: 0.1
    use_dual_attention: true

  # Fusion layer
  fusion:
    hidden_dim: 256
    dropout: 0.1
    use_gating: true

  # Output configuration
  output:
    predict_direction: true
    n_classes: 3  # Up, Down, Neutral

# Training Configuration
training:
  # Basic settings
  batch_size: 64
  epochs: 100
  early_stopping_patience: 15

  # Optimization
  optimizer: "adamw"  # adam, adamw, sgd
  learning_rate: 0.001
  weight_decay: 1e-5
  gradient_clip: 1.0

  # Learning rate scheduling
  scheduler:
    type: "reduce_on_plateau"  # reduce_on_plateau, cosine, step
    patience: 5
    factor: 0.5
    min_lr: 1e-6

  # Loss weights (for multi-task learning)
  loss_weights:
    price: 1.0
    direction: 0.5

  # Data augmentation
  augmentation:
    enabled: true
    noise_level: 0.01
    dropout_rate: 0.1

  # Mixed precision training
  mixed_precision: false

  # Distributed training
  distributed:
    enabled: false
    backend: "nccl"
    world_size: 1

# Reinforcement Learning Configuration
rl:
  enabled: false  # Set to true to enable RL training

  # PPO parameters
  ppo:
    learning_rate: 3e-4
    n_steps: 2048
    n_epochs: 10
    batch_size: 64
    gamma: 0.99
    gae_lambda: 0.95
    clip_epsilon: 0.2

  # Trading environment
  environment:
    initial_balance: 100000
    max_position_size: 1.0
    transaction_cost: 0.001
    slippage: 0.0005
    max_drawdown: 0.20

  # Training settings
  n_iterations: 1000
  eval_interval: 50

# Evaluation Configuration
evaluation:
  # Metrics to track
  metrics:
    - rmse
    - mae
    - directional_accuracy
    - sharpe_ratio
    - max_drawdown
    - profit_factor

  # Backtesting
  backtest:
    initial_capital: 100000
    position_size: 0.1  # 10% of capital per trade
    stop_loss: 0.05
    take_profit: 0.10

  # Monte Carlo simulation
  monte_carlo:
    enabled: false
    n_simulations: 1000
    confidence_level: 0.95

# Experiment Tracking
tracking:
  # Weights & Biases
  wandb:
    enabled: false
    project: "hybrid-trading-model"
    entity: null
    tags: ["hybrid", "deep-learning", "trading"]

  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "runs/hybrid_model"

  # MLflow
  mlflow:
    enabled: false
    tracking_uri: "http://localhost:5000"
    experiment_name: "hybrid_trading"

# Checkpointing
checkpointing:
  save_dir: "checkpoints"
  save_interval: 10  # Save every N epochs
  keep_best_only: true
  monitor_metric: "val_sharpe_ratio"  # or "val_loss"
  mode: "max"  # max for sharpe_ratio, min for loss

# Hyperparameter Optimization
hyperopt:
  enabled: false

  # Optuna settings
  optuna:
    n_trials: 100
    study_name: "hybrid_model_optimization"
    direction: "maximize"  # maximize sharpe ratio

  # Search space
  search_space:
    learning_rate: [1e-5, 1e-2]
    batch_size: [16, 128]
    lstm_hidden: [128, 512]
    transformer_layers: [2, 6]
    dropout: [0.1, 0.5]

# Hardware Configuration
hardware:
  # Device settings
  device: "auto"  # auto, cuda, cpu
  gpu_ids: [0]  # List of GPU IDs to use

  # Memory management
  gradient_accumulation_steps: 1
  pin_memory: true
  num_workers: 4

  # RunPod specific
  runpod:
    enabled: false
    instance_type: "RTX A6000"
    spot_instance: true

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "training/training.log"
  console: true

# Random Seeds (for reproducibility)
seeds:
  python: 42
  numpy: 42
  torch: 42
  cuda_deterministic: true